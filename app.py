import os
import numpy as np
import pandas as pd
import streamlit as st
from sklearn.model_selection import train_test_split
import inspect
from collections import Counter
import seaborn as sns
import matplotlib.pyplot as plt

# classification models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import VotingClassifier, StackingClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

# Keras/TensorFlow –¥–ª—è –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞
import keras
from tensorflow.keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import to_categorical
from scikeras.wrappers import KerasClassifier

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã Streamlit
st.set_page_config(page_title="HSE | –¢–ò–ò–ü–ê", layout="wide")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è session_state
def initialize_session_state():
    if 'df' not in st.session_state:
        st.session_state.df = None
        st.session_state.backup_df = None
        st.session_state.X_train = None
        st.session_state.X_test = None
        st.session_state.y_train = None
        st.session_state.y_test = None
        st.session_state.train_ids = None
        st.session_state.test_ids = None
        st.session_state.models = {}
        st.session_state.scaler = None
        st.session_state.onehot_encoder = None
        st.session_state.label_encoders = {}
        st.session_state.target_col = None
        st.session_state.id_cols = []
        st.session_state.selected_features = []
        st.session_state.norm_cols = []
        st.session_state.log_cols = []
        st.session_state.dummy_cols = []
        st.session_state.balance_method = "–ù–µ—Ç"
        st.session_state.method = "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –º–µ–¥–∏–∞–Ω—É"
        st.session_state.sep_sign = ";"
        st.session_state.decimal_sign = ","
        st.session_state.ensemble_model = None
        st.session_state.preprocess_params = {}

initialize_session_state()

# –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
st.title("¬© HSE | –¢–ò–ò–ü–ê")
st.markdown("**Fast-touch classification-analytical tool**")
st.markdown("---")

# –ë–ª–æ–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–º–∞–Ω–¥–µ
with st.expander("‚Ñπ –û –∫–æ–º–∞–Ω–¥–µ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤"):
    st.markdown("""
    **–ö–æ–º–∞–Ω–¥–∞ –ø—Ä–æ–µ–∫—Ç–∞:**
    - –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏–Ω –ò–ª—å—è—â–µ–Ω–∫–æ ‚Äî Team Leader
    - –í–∞–¥–∏–º –ö–∞–∑–∞–∫–æ–≤ ‚Äî Master of Machine Learning
    - –ê–Ω–¥—Ä–µ–π –®–∏—Ä—à–æ–≤ ‚Äî Business Developer
    - –¢–∞—Ç—å—è–Ω–∞ –ß–µ—Ä–Ω—ã—Ö ‚Äî Designer
    """)

# –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
with st.expander("–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —Å–µ—Ä–≤–∏—Å–∞", expanded=True):
    st.markdown("""
    1. *–í—ã–±–µ—Ä–∏—Ç–µ –Ω—É–∂–Ω—É—é —Ç–µ–º—É* –∞–Ω–∞–ª–∏–∑–∞
    2. *–ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –¥–∞–Ω–Ω—ã–µ* —Å –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    3. *–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ* –∏ –≤—ã–±–µ—Ä–∏—Ç–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
    4. *–ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–Ω–∞–ª–∏–∑* ‚Äî –≤—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –¥–æ–≤–µ—Ä—å—Ç–µ—Å—å –Ω–∞–º. *–î–∞–π—Ç–µ –∞–ª–≥–æ—Ä–∏—Ç–º—É –≤—Ä–µ–º—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.*
    5. *–ò–∑—É—á–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã* ‚Äî –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.
    6. *–í—ã–±–µ—Ä–∏—Ç–µ* –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –∏ *–ø—Ä–∏–º–µ–Ω–∏—Ç–µ* –µ—ë –Ω–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    """)

st.markdown("---")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
def load_data(uploaded_file):
    try:
        file_name = uploaded_file.name.lower()
        
        if file_name.endswith('.csv'):
            sep_sign = st.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å",
                (";", ",", " ", "|"), index=0)
            st.session_state.sep_sign = sep_sign

            decimal_sign = st.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ –æ—Ç–¥–µ–ª–∏—Ç–µ–ª—å –¥—Ä–æ–±–Ω–æ–π —á–∞—Å—Ç–∏",
                (".", ","), index=1)
            st.session_state.decimal_sign = decimal_sign

            df = pd.read_csv(uploaded_file, sep=sep_sign, decimal=decimal_sign)
        elif file_name.endswith(('.xls', '.xlsx')):
            df = pd.read_excel(uploaded_file)
        
        st.session_state.df = df.copy()
        st.session_state.backup_df = df.copy()
        st.success("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
        return df
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        return None

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
def handle_missing_values(df, option):
    df_processed = df.copy()
    
    for col in df_processed.columns:
        if pd.api.types.is_numeric_dtype(df_processed[col]):
            if option == "–£–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏":
                df_processed = df_processed.dropna(subset=[col])
            elif option == "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –Ω–æ–ª—å":
                df_processed[col] = df_processed[col].fillna(0)
            elif option == "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –º–µ–¥–∏–∞–Ω—É":
                df_processed[col] = df_processed[col].fillna(df_processed[col].median())
            elif option == "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Å—Ä–µ–¥–Ω–µ–µ":
                df_processed[col] = df_processed[col].fillna(df_processed[col].mean())
            elif option == "–ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è":
                df_processed[col] = df_processed[col].interpolate()
        else:
            if option == "–£–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏":
                df_processed = df_processed.dropna(subset=[col])
            elif option == "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –º–æ–¥—É":
                if not df_processed[col].mode().empty:
                    df_processed[col] = df_processed[col].fillna(df_processed[col].mode()[0])
                else:
                    df_processed[col] = df_processed[col].fillna("Unknown")
            elif option == "–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é 'Unknown'":
                df_processed[col] = df_processed[col].fillna("Unknown")
            elif option == "–≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è (–ø–æ—Å–ª–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)":
                df_processed[col] = df_processed[col].fillna(method='ffill')
    
    return df_processed

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
def preprocess_data(data, target_col, id_cols, features, norm_cols, log_cols, dummy_cols,
                   balance_method, test_size, random_state):
    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
        st.session_state.preprocess_params = {
            'target_col': target_col,
            'features': features,
            'norm_cols': norm_cols,
            'log_cols': log_cols,
            'dummy_cols': dummy_cols,
            'balance_method': balance_method
        }
        
        X = data[features].copy()
        y = data[target_col].copy()
        ids = data[id_cols].copy() if id_cols else None

        # –ö–æ–¥–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π —Ç–∞—Ä–≥–µ—Ç
        #if not pd.api.types.is_numeric_dtype(y):
            #le = LabelEncoder()
            #y = le.fit_transform(y)
            #st.session_state.label_encoders['target'] = le

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y,
            test_size=test_size/100,
            random_state=random_state,
            stratify=y if not pd.api.types.is_numeric_dtype(y) else None
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º ID
        if ids is not None:
            train_ids = ids.loc[X_train.index]
            test_ids = ids.loc[X_test.index]
        else:
            train_ids, test_ids = X_train.index, X_test.index

        # 1. –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏–µ
        for col in log_cols:
            if col in X_train.columns:
                if (X_train[col] <= 0).any():
                    st.warning(f"–ö–æ–ª–æ–Ω–∫–∞ {col} —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω—É–ª–∏/–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è! –î–æ–±–∞–≤–ª–µ–Ω–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞ 1 –ø–µ—Ä–µ–¥ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏–µ–º.")
                    X_train[col] = np.log1p(X_train[col] + 1)
                    X_test[col] = np.log1p(X_test[col] + 1)
                else:
                    X_train[col] = np.log1p(X_train[col])
                    X_test[col] = np.log1p(X_test[col])

        # 2. One-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if dummy_cols:
            encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')
            train_encoded = encoder.fit_transform(X_train[dummy_cols])
            test_encoded = encoder.transform(X_test[dummy_cols])

            encoded_cols = encoder.get_feature_names_out(dummy_cols)
            train_encoded_df = pd.DataFrame(train_encoded, columns=encoded_cols, index=X_train.index)
            test_encoded_df = pd.DataFrame(test_encoded, columns=encoded_cols, index=X_test.index)

            X_train = X_train.drop(columns=dummy_cols).join(train_encoded_df)
            X_test = X_test.drop(columns=dummy_cols).join(test_encoded_df)
            st.session_state.onehot_encoder = encoder

        # 3. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        if norm_cols:
            scaler = StandardScaler()
            X_train[norm_cols] = scaler.fit_transform(X_train[norm_cols])
            X_test[norm_cols] = scaler.transform(X_test[norm_cols])
            st.session_state.scaler = scaler

        # 4. –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
        if balance_method != "–ù–µ—Ç" and not pd.api.types.is_numeric_dtype(y_train):
            if balance_method == "Random Oversampling":
                sampler = RandomOverSampler(random_state=random_state)
            elif balance_method == "SMOTE":
                sampler = SMOTE(random_state=random_state)
            elif balance_method == "Random Undersampling":
                sampler = RandomUnderSampler(random_state=random_state)
            
            X_train, y_train = sampler.fit_resample(X_train, y_train)
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
            pd.Series(y).value_counts().plot(kind='bar', ax=ax1, title='–î–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏')
            pd.Series(y_train).value_counts().plot(kind='bar', ax=ax2, title='–ü–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏')
            st.pyplot(fig)

        return X_train, X_test, y_train, y_test, train_ids, test_ids
    
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        return None, None, None, None, None, None

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
def process_new_data(new_df):
    try:
        params = st.session_state.preprocess_params
        if not params:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ!")
        
        X_new = new_df[params['features']].copy()
        
        # 1. –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏–µ
        for col in params['log_cols']:
            if col in X_new.columns:
                X_new[col] = np.log1p(X_new[col] + 1)
        
        # 2. One-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if 'onehot_encoder' in st.session_state and params['dummy_cols']:
            encoded = st.session_state.onehot_encoder.transform(X_new[params['dummy_cols']])
            encoded_cols = st.session_state.onehot_encoder.get_feature_names_out(params['dummy_cols'])
            encoded_df = pd.DataFrame(encoded, columns=encoded_cols, index=X_new.index)
            X_new = X_new.drop(params['dummy_cols'], axis=1).join(encoded_df)
        
        # 3. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        if 'scaler' in st.session_state and params['norm_cols']:
            X_new[params['norm_cols']] = st.session_state.scaler.transform(X_new[params['norm_cols']])
        
        return X_new
    
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        return None

# –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
st.title("–î–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª (CSV –∏–ª–∏ Excel)", type=["csv", "xls", "xlsx"])

if uploaded_file is not None:
    df = load_data(uploaded_file)
    if df is not None:
        st.dataframe(df.head())

col1, col2 = st.columns(2)
with col1:
    method = st.selectbox(
        "–ú–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤:",
        options=[
            "–£–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏",
            "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –Ω–æ–ª—å",
            "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –º–µ–¥–∏–∞–Ω—É",
            "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Å—Ä–µ–¥–Ω–µ–µ",
            "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –º–æ–¥—É",
            "–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é 'Unknown'",
            "–ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è",
            "–≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è (–ø–æ—Å–ª–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)"
        ],
        index=2,
        help="–í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π",
        key="missing_method_select"
    )
    st.session_state.method = method # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥

with col2:
    show_details = st.checkbox("–ü–æ–∫–∞–∑–∞—Ç—å –ø–æ—è—Å–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤", value=True)

if show_details:
    with st.expander("üìö –ü–æ—è—Å–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏"):
        st.markdown("""
        - **–£–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏** - –ø–æ–ª–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –ø—Ä–æ–ø—É—Å–∫–∏
        - **–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –Ω–æ–ª—å** - –∑–∞–º–µ–Ω–∞ –≤—Å–µ—Ö –ø—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–∞ 0 (–¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö)
        - **–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –º–µ–¥–∏–∞–Ω—É** - –∑–∞–º–µ–Ω–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –º–µ–¥–∏–∞–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º (—É—Å—Ç–æ–π—á–∏–≤–æ –∫ –≤—ã–±—Ä–æ—Å–∞–º)
        - **–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Å—Ä–µ–¥–Ω–µ–µ** - –∑–∞–º–µ–Ω–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º (–º–æ–∂–µ—Ç –∏—Å–∫–∞–∂–∞—Ç—å—Å—è –≤—ã–±—Ä–æ—Å–∞–º–∏)
        - **–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –º–æ–¥—É** - –∑–∞–º–µ–Ω–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º (–¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)
        - **–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é 'Unknown'** - –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–ø—É—Å–∫–µ
        - **–ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è** - –ª–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π (–¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤)
        - **–≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è** - –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏–º –∏–∑–≤–µ—Å—Ç–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º (–º–µ—Ç–æ–¥ 'forward fill')
        """)

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
if st.checkbox("–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è", key="process_missing_checkbox"):
    with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö..."):
        df = handle_missing_values(df, method)
        st.session_state.df = df.copy() # –û–±–Ω–æ–≤–ª—è–µ–º df –≤ session_state
        st.success("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        st.markdown("---")
        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏")

        col1, col2 = st.columns(2)
        with col1:
            st.metric("–ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫", len(st.session_state.backup_df))
        with col2:
            st.metric("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏", len(st.session_state.df))

# –ü–æ–∫–∞–∑–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
st.dataframe(st.session_state.df)


# –í–∏–∑—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
if st.session_state.df is not None:
    st.title("–í–∏–∑—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö")
    st.subheader("–û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")

    desc = st.session_state.df.describe().T
    desc['missing'] = st.session_state.df.isna().sum()
    desc['missing_percent'] = (desc['missing'] / len(st.session_state.df)).round(2)
    desc['dtype'] = st.session_state.df.dtypes
    desc['unique'] = st.session_state.df.nunique()

    st.dataframe(
        desc.style.format({
            'count': '{:.1f}',
            'mean': '{:.2f}',
            'std': '{:.2f}',
            'min': '{:.2f}',
            '25%': '{:.2f}',
            '50%': '{:.2f}',
            '75%': '{:.2f}',
            'max': '{:.2f}',
            'missing_percent': '{:.0%}'
        }),
        use_container_width=True
    )
    # –í—ã–±–æ—Ä —Ç–∏–ø–∞ –≥—Ä–∞—Ñ–∏–∫–∞
    chart_type = st.radio(
    "–¢–∏–ø –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:",
    ["–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞", "–Ø—â–∏–∫ —Å —É—Å–∞–º–∏ (Boxplot)", "–ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"],
    horizontal=True
)

    # –ì—Ä–∞—Ñ–∏–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    col1, col2 = st.columns([3, 1])
    with col1:
        numeric_cols_for_plot = st.session_state.df.select_dtypes(include=['number']).columns
        if len(numeric_cols_for_plot) > 0:
            selected_col = st.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:",
                options=numeric_cols_for_plot,
                index=0
            )
        else:
            st.warning("–ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è.")
            selected_col = None
    
    with col2:
        bins = 20 # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if chart_type == "–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞" and selected_col:
            bins = st.slider(
                "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤:",
                min_value=5,
                max_value=50,
                value=int(np.sqrt(len(st.session_state.df)) if len(st.session_state.df) > 0 else 20)
            )
    
    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞
    if selected_col:
        plt.style.use('dark_background')
        fig, ax = plt.subplots(figsize=(8, 6))
    
        if chart_type == "–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞":
            sns.histplot(st.session_state.df[selected_col], bins=bins, kde=True, ax=ax)
            ax.set_title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {selected_col}')
            ax.set_xlabel(selected_col)
            ax.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
        elif chart_type == "–Ø—â–∏–∫ —Å —É—Å–∞–º–∏ (Boxplot)":
            sns.boxplot(x=st.session_state.df[selected_col], ax=ax)
            ax.set_title(f'Boxplot –¥–ª—è {selected_col}')
        elif chart_type == "–ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è":
            sns.kdeplot(st.session_state.df[selected_col], ax=ax, fill=True)
            ax.set_title(f'–ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è {selected_col}')
            ax.set_xlabel(selected_col)
            ax.set_ylabel('–ü–ª–æ—Ç–Ω–æ—Å—Ç—å')
    
        st.pyplot(fig, clear_figure=True)
    
    st.markdown("---")
    
    st.subheader("Scatter plot")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        x_axis = st.selectbox(
            "–û—Å—å X",
            options=st.session_state.df.columns,
            index=0
        )
    with col2:
        y_axis = st.selectbox(
            "–û—Å—å Y",
            options=st.session_state.df.columns,
            index=1 if len(st.session_state.df.columns) > 1 else 0
        )
    
    with col3:
        color_col = st.selectbox(
            "–¶–≤–µ—Ç",
            options=["None"] + list(st.session_state.df.columns),
            index=0
        )
    with col4:
        size_col = st.selectbox(
            "–†–∞–∑–º–µ—Ä",
            options=["None"] + list(st.session_state.df.columns),
            index=0
        )
    
    if (color_col == "None") and (size_col == "None"):
        st.scatter_chart(st.session_state.df, x=x_axis, y=y_axis)
    elif (size_col == "None") and (color_col != "None"):
        st.scatter_chart(st.session_state.df, x=x_axis, y=y_axis, color=color_col)
    elif (size_col != "None") and (color_col == "None"):
        st.scatter_chart(st.session_state.df, x=x_axis, y=y_axis, size = size_col)
    elif (size_col != "None") and (color_col != "None"):
        st.scatter_chart(st.session_state.df, x=x_axis, y=y_axis, color = color_col, size = size_col)

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è
if st.session_state.df is not None:
    st.title("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è")
    
    target_col = st.selectbox(
        "–í—ã–±–µ—Ä–∏—Ç–µ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é:",
        options=st.session_state.df.columns,
        index=0
    )
    st.session_state.target_col = target_col

    # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–ª–∞—Å—Å–∞–º
    if st.session_state.target_col:
        st.subheader(f"–ê–Ω–∞–ª–∏–∑ –ø–æ –∫–ª–∞—Å—Å–∞–º ({st.session_state.target_col})")
        class_stats = st.session_state.df.groupby(st.session_state.target_col,as_index=False).mean().merge(pd.DataFrame(st.session_state.df[st.session_state.target_col].value_counts()).reset_index(), how='left')
        st.dataframe(class_stats)
        

    id_cols = st.multiselect(
        "–í—ã–±–µ—Ä–∏—Ç–µ ID-—Å—Ç–æ–ª–±—Ü—ã (–Ω–µ –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤ –º–æ–¥–µ–ª–∏):",
        options=[col for col in st.session_state.df.columns if col != target_col]
    )
    st.session_state.id_cols = id_cols

    available_features = [col for col in st.session_state.df.columns if col != target_col and col not in id_cols]
    selected_features = st.multiselect(
        "–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏:",
        options=available_features,
        default=available_features
    )
    st.session_state.selected_features = selected_features

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
    with st.expander("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏"):
        numeric_cols = [col for col in selected_features if pd.api.types.is_numeric_dtype(st.session_state.df[col])]
        
        st.subheader("–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è")
        norm_cols = st.multiselect(
            "–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:",
            options=numeric_cols
        )
        st.session_state.norm_cols = norm_cols

        st.subheader("–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏–µ")
        log_cols = st.multiselect(
            "–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏—è:",
            options=numeric_cols
        )
        st.session_state.log_cols = log_cols

        st.subheader("–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
        categorical_cols = [col for col in selected_features if not pd.api.types.is_numeric_dtype(st.session_state.df[col])]
        dummy_cols = st.multiselect(
            "–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è one-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è:",
            options=categorical_cols
        )
        st.session_state.dummy_cols = dummy_cols

        st.subheader("–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤")
        if not pd.api.types.is_numeric_dtype(st.session_state.df[target_col]):
            balance_method = st.selectbox(
                "–ú–µ—Ç–æ–¥ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏:",
                options=["–ù–µ—Ç", "Random Oversampling", "SMOTE", "Random Undersampling"]
            )
            st.session_state.balance_method = balance_method
        else:
            st.warning("–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö")

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∞–∑–±–∏–µ–Ω–∏—è
    test_size = st.slider(
        "–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (%)",
        min_value=5,
        max_value=40,
        value=20,
        step=5
    )
    
    random_state = st.number_input(
        "Random state –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏",
        min_value=0,
        value=42
    )

    if st.button("–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ"):
        with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö..."):
            X_train, X_test, y_train, y_test, train_ids, test_ids = preprocess_data(
                st.session_state.df,
                st.session_state.target_col,
                st.session_state.id_cols,
                st.session_state.selected_features,
                st.session_state.norm_cols,
                st.session_state.log_cols,
                st.session_state.dummy_cols,
                st.session_state.balance_method,
                test_size,
                random_state
            )
            
            if X_train is not None:
                st.session_state.X_train = X_train
                st.session_state.X_test = X_test
                st.session_state.y_train = y_train
                st.session_state.y_test = y_test
                st.session_state.train_ids = train_ids
                st.session_state.test_ids = test_ids
                
                st.success("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã!")
                
                # –ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏")
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞", f"{len(X_train)} —Å—Ç—Ä–æ–∫")
                    st.write("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
                    st.write(pd.Series(y_train).value_counts())
                    st.dataframe(pd.concat([train_ids, X_train, y_train], axis=1))
                with col2:
                    st.metric("–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞", f"{len(X_test)} —Å—Ç—Ä–æ–∫")
                    st.write("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
                    st.write(pd.Series(y_test).value_counts())
                    st.dataframe(pd.concat([test_ids, X_test, y_test], axis=1))
            else:
                st.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö")
def plot_confusion_matrix(y_true, y_pred, title):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫"""
    cm = confusion_matrix(y_true, y_pred)
    fig, ax = plt.subplots(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)
    ax.set_title(title, pad=20)
    ax.set_xlabel('Predicted labels')
    ax.set_ylabel('True labels')
    st.pyplot(fig, clear_figure=True)

def evaluate_model(model, X_train, X_test, y_train, y_test, model_name, use_cv=False, cv_folds=5):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤ session_state"""
    try:
        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        if use_cv:
            cv_scores = cross_val_score(model, X_train, y_train, cv=cv_folds, scoring='accuracy')
            st.write(f"–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è (—Å—Ä–µ–¥–Ω–µ–µ accuracy): {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model.fit(X_train, y_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)
        
        # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
        metrics = {
            'Accuracy': [
                accuracy_score(y_train, y_train_pred),
                accuracy_score(y_test, y_test_pred)
            ],
            'Precision': [
                precision_score(y_train, y_train_pred, average='weighted'),
                precision_score(y_test, y_test_pred, average='weighted')
            ],
            'Recall': [
                recall_score(y_train, y_train_pred, average='weighted'),
                recall_score(y_test, y_test_pred, average='weighted')
            ],
            'F1': [
                f1_score(y_train, y_train_pred, average='weighted'),
                f1_score(y_test, y_test_pred, average='weighted')
            ]
        }
        
        metrics_df = pd.DataFrame(metrics, index=['Train', 'Test'])
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü –æ—à–∏–±–æ–∫
        st.subheader(f"–ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –¥–ª—è {model_name}")
        col1, col2 = st.columns(2)
        with col1:
            plot_confusion_matrix(y_train, y_train_pred, f'Train: {model_name}')
        with col2:
            plot_confusion_matrix(y_test, y_test_pred, f'Test: {model_name}')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ session_state
        if 'models' not in st.session_state:
            st.session_state.models = {}
        st.session_state.models[model_name] = model
        
        return model, metrics_df, y_train_pred, y_test_pred
    
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–∏ {model_name}: {str(e)}")
        return None, None, None, None

# –ú–æ–¥–µ–ª—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
def logistic_regression(X_train, X_test, y_train, y_test, use_cv=False, cv_folds=5):
    model = LogisticRegression(
        max_iter=1000,
        random_state=42,
        multi_class='multinomial'
    )
    return evaluate_model(model, X_train, X_test, y_train, y_test, 
                         "Logistic Regression", use_cv, cv_folds)

# –ú–æ–¥–µ–ª—å –¥–µ—Ä–µ–≤–∞ —Ä–µ—à–µ–Ω–∏–π
def decision_tree(X_train, X_test, y_train, y_test, max_depth=10, min_samples_split=10, use_cv=False, cv_folds=5):
    model = DecisionTreeClassifier(
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        random_state=42
    )
    return evaluate_model(model, X_train, X_test, y_train, y_test,
                         "Decision Tree", use_cv, cv_folds)

# –ú–æ–¥–µ–ª—å —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞
def random_forest(X_train, X_test, y_train, y_test, n_estimators=50, max_depth=10, 
                 min_samples_split=10, use_cv=False, cv_folds=5):
    model = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        random_state=42
    )
    return evaluate_model(model, X_train, X_test, y_train, y_test,
                         "Random Forest", use_cv, cv_folds)

# –ú–æ–¥–µ–ª—å XGBoost
def xgboost_model(X_train, X_test, y_train, y_test, learning_rate=0.01, n_estimators=50, 
                 max_depth=5, use_cv=False, cv_folds=5):
    # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ –¥–ª—è XGBoost
    le = LabelEncoder()
    y_train_encoded = le.fit_transform(y_train)
    y_test_encoded = le.transform(y_test)
    
    model = XGBClassifier(
        learning_rate=learning_rate,
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=42,
        use_label_encoder=False,
        eval_metric='mlogloss'
    )
    
    model, metrics_df, _, _ = evaluate_model(model, X_train, X_test, 
                                           y_train_encoded, y_test_encoded,
                                           "XGBoost", use_cv, cv_folds)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –º–µ—Ç–æ–∫
    st.session_state.label_encoder_xgboost = le
    
    return model, metrics_df, None, None

# –ú–æ–¥–µ–ª—å KNN
def knn_classifier(X_train, X_test, y_train, y_test, n_neighbors=5, use_cv=False, cv_folds=5):
    model = KNeighborsClassifier(n_neighbors=n_neighbors)
    return evaluate_model(model, X_train, X_test, y_train, y_test,
                         "KNN", use_cv, cv_folds)

# –ú–æ–¥–µ–ª—å SVM
def svm_classifier(X_train, X_test, y_train, y_test, use_cv=False, cv_folds=5):
    model = SVC(random_state=42, probability=True)
    return evaluate_model(model, X_train, X_test, y_train, y_test,
                         "SVM", use_cv, cv_folds)

# –ú–æ–¥–µ–ª—å –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞
def perceptron_classifier(X_train, X_test, y_train, y_test, layers=1, neurons=64, 
                         learning_rate=0.01, epochs=10, use_cv=False, cv_folds=5):
    # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ –¥–ª—è –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞
    le = LabelEncoder()
    y_train_encoded = le.fit_transform(y_train)
    y_test_encoded = le.transform(y_test)
    num_classes = len(le.classes_)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∫–∏ –≤ one-hot encoding
    y_train_onehot = to_categorical(y_train_encoded, num_classes=num_classes)
    y_test_onehot = to_categorical(y_test_encoded, num_classes=num_classes)
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = Sequential()
    model.add(Dense(neurons, input_dim=X_train.shape[1], activation='relu'))
    
    for _ in range(layers - 1):
        model.add(Dense(neurons, activation='relu'))
    
    model.add(Dense(num_classes, activation='softmax'))
    
    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    history = model.fit(
        X_train, y_train_onehot,
        validation_data=(X_test, y_test_onehot),
        epochs=epochs,
        batch_size=32,
        verbose=0
    )
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_train_pred = np.argmax(model.predict(X_train, verbose=0), axis=1)
    y_test_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)
    
    # –û–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
    y_train_pred = le.inverse_transform(y_train_pred)
    y_test_pred = le.inverse_transform(y_test_pred)
    
    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
    metrics = {
        'Accuracy': [
            accuracy_score(y_train, y_train_pred),
            accuracy_score(y_test, y_test_pred)
        ],
        'Precision': [
            precision_score(y_train, y_train_pred, average='weighted'),
            precision_score(y_test, y_test_pred, average='weighted')
        ],
        'Recall': [
            recall_score(y_train, y_train_pred, average='weighted'),
            recall_score(y_test, y_test_pred, average='weighted')
        ],
        'F1': [
            f1_score(y_train, y_train_pred, average='weighted'),
            f1_score(y_test, y_test_pred, average='weighted')
        ]
    }
    
    metrics_df = pd.DataFrame(metrics, index=['Train', 'Test'])
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    st.subheader(f"–ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –¥–ª—è Perceptron")
    col1, col2 = st.columns(2)
    with col1:
        plot_confusion_matrix(y_train, y_train_pred, 'Train: Perceptron')
    with col2:
        plot_confusion_matrix(y_test, y_test_pred, 'Test: Perceptron')
    
    # –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è
    st.subheader("–ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è")
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    ax1.plot(history.history['accuracy'], label='Train')
    ax1.plot(history.history['val_accuracy'], label='Validation')
    ax1.set_title('Accuracy')
    ax1.legend()
    
    ax2.plot(history.history['loss'], label='Train')
    ax2.plot(history.history['val_loss'], label='Validation')
    ax2.set_title('Loss')
    ax2.legend()
    
    st.pyplot(fig, clear_figure=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫
    st.session_state.perceptron_model = model
    st.session_state.label_encoder_perceptron = le
    
    return model, metrics_df, y_train_pred, y_test_pred

# –§—É–Ω–∫—Ü–∏–∏ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
def create_voting_ensemble(models, X_train, X_test, y_train, y_test):
    """–°–æ–∑–¥–∞–µ—Ç –∞–Ω—Å–∞–º–±–ª—å –º–µ—Ç–æ–¥–æ–º –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è"""
    voting = VotingClassifier(
        estimators=[(name, model) for name, model in models.items()],
        voting='hard'
    )
    return evaluate_model(voting, X_train, X_test, y_train, y_test, "Voting Ensemble")

def create_stacking_ensemble(models, X_train, X_test, y_train, y_test):
    """–°–æ–∑–¥–∞–µ—Ç –∞–Ω—Å–∞–º–±–ª—å –º–µ—Ç–æ–¥–æ–º —Å—Ç—ç–∫–∏–Ω–≥–∞"""
    stacking = StackingClassifier(
        estimators=[(name, model) for name, model in models.items()],
        final_estimator=LogisticRegression(max_iter=1000),
        cv=5
    )
    return evaluate_model(stacking, X_train, X_test, y_train, y_test, "Stacking Ensemble")

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
def ensemble_interface(X_train, X_test, y_train, y_test):
    st.title("–ê–Ω—Å–∞–º–±–ª–∏ –º–æ–¥–µ–ª–µ–π")
    
    if 'models' not in st.session_state or not st.session_state.models:
        st.warning("–°–Ω–∞—á–∞–ª–∞ –ø–æ—Å—Ç—Ä–æ–π—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –º–æ–¥–µ–ª—å")
        return
    
    available_models = st.session_state.models
    selected_models = st.multiselect(
        "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è",
        options=list(available_models.keys()),
        default=list(available_models.keys())
    )
    
    if len(selected_models) < 2:
        st.warning("–î–ª—è –∞–Ω—Å–∞–º–±–ª—è –Ω—É–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å —Ö–æ—Ç—è –±—ã 2 –º–æ–¥–µ–ª–∏")
        return
    
    ensemble_type = st.selectbox(
        "–¢–∏–ø –∞–Ω—Å–∞–º–±–ª—è",
        ["–ì–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ", "–°—Ç—ç–∫–∏–Ω–≥"]
    )
    
    if st.button("–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∞–Ω—Å–∞–º–±–ª—å"):
        models_to_ensemble = {name: available_models[name] for name in selected_models}
        
        with st.spinner("–°—Ç—Ä–æ–∏–º –∞–Ω—Å–∞–º–±–ª—å..."):
            if ensemble_type == "–ì–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ":
                model, metrics, _, _ = create_voting_ensemble(
                    models_to_ensemble, X_train, X_test, y_train, y_test
                )
            else:
                model, metrics, _, _ = create_stacking_ensemble(
                    models_to_ensemble, X_train, X_test, y_train, y_test
                )
            
            st.session_state.ensemble_model = model
            st.success("–ê–Ω—Å–∞–º–±–ª—å —É—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω!")
            
            st.subheader("–ú–µ—Ç—Ä–∏–∫–∏ –∞–Ω—Å–∞–º–±–ª—è")
            st.dataframe(metrics)

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
def prediction_interface():
    st.title("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    
    uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (CSV –∏–ª–∏ Excel)", 
                                   type=["csv", "xlsx"])
    
    if not uploaded_file:
        return
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    try:
        if uploaded_file.name.endswith('.csv'):
            new_data = pd.read_csv(uploaded_file, sep=st.session_state.sep_sign, 
                                 decimal=st.session_state.decimal_sign)
        else:
            new_data = pd.read_excel(uploaded_file)
        
        st.success(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(new_data)} –∑–∞–ø–∏—Å–µ–π")
        st.dataframe(new_data.head())
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        return
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    try:
        X_new = process_new_data(new_data)
        if X_new is None:
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ")
            return
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        return
    
    # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    available_models = {}
    if 'models' in st.session_state:
        available_models.update(st.session_state.models)
    if 'ensemble_model' in st.session_state:
        available_models['Ensemble'] = st.session_state.ensemble_model
    
    if not available_models:
        st.warning("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
        return
    
    selected_models = st.multiselect(
        "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è",
        options=list(available_models.keys()),
        default=list(available_models.keys())
    )
    if st.button("–í—ã–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"):
        predictions = pd.DataFrame()
        
        for model_name in selected_models:
            model = available_models[model_name]
            
            try:
                # –û—Å–æ–±—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π, —Ç—Ä–µ–±—É—é—â–∏—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
                if model_name == 'XGBoost' and 'label_encoder_xgboost' in st.session_state:
                    le = st.session_state.label_encoder_xgboost
                    pred = le.inverse_transform(model.predict(X_new))
                elif model_name == 'Perceptron' and 'label_encoder_perceptron' in st.session_state:
                    le = st.session_state.label_encoder_perceptron
                    pred_proba = model.predict(X_new)
                    pred = le.inverse_transform(np.argmax(pred_proba, axis=1))
                else:
                    pred = model.predict(X_new)
                
                predictions[model_name] = pred
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ —Å –ø–æ–º–æ—â—å—é {model_name}: {str(e)}")
                continue
        
        # –ì–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ, –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π
        if len(selected_models) > 1:
            predictions['Majority_Vote'] = predictions.mode(axis=1)[0]
        
        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
        st.dataframe(predictions)
        
        # –ö–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        csv = predictions.to_csv(index=False).encode('utf-8')
        st.download_button(
            "–°–∫–∞—á–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è",
            data=csv,
            file_name='predictions.csv',
            mime='text/csv'
        )

# –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –º–æ–¥–µ–ª–µ–π
def models_interface(X_train, X_test, y_train, y_test):
    st.title("–ú–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")
    
    use_cv = st.sidebar.checkbox("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é", value=False)
    cv_folds = st.sidebar.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤", 2, 10, 5) if use_cv else 5
    
    # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Ç–∞–±—ã
    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
        "Logistic Regression", "Decision Tree", "Random Forest", 
        "XGBoost", "KNN", "SVM", "Perceptron"
    ])
    
    with tab1:
        st.subheader("–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è")
        if st.button("–û–±—É—á–∏—Ç—å Logistic Regression"):
            with st.spinner("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏..."):
                model, metrics, _, _ = logistic_regression(
                    X_train, X_test, y_train, y_test, use_cv, cv_folds
                )
                st.dataframe(metrics)
    
    with tab2:
        st.subheader("–î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π")
        max_depth = st.number_input("max_depth", 1, 50, 10)
        min_samples = st.number_input("min_samples_split", 2, 20, 10)
        if st.button("–û–±—É—á–∏—Ç—å Decision Tree"):
            with st.spinner("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏..."):
                model, metrics, _, _ = decision_tree(
                    X_train, X_test, y_train, y_test, 
                    max_depth, min_samples, use_cv, cv_folds
                )
                st.dataframe(metrics)
    
    with tab3:
        st.subheader("–°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å")
        n_estimators = st.number_input("n_estimators", 10, 500, 50)
        max_depth = st.number_input("max_depth", 1, 50, 10)
        min_samples = st.number_input("min_samples_split", 2, 20, 10)
        if st.button("–û–±—É—á–∏—Ç—å Random Forest"):
            with st.spinner("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏..."):
                model, metrics, _, _ = random_forest(
                    X_train, X_test, y_train, y_test,
                    n_estimators, max_depth, min_samples, use_cv, cv_folds
                )
                st.dataframe(metrics)
    
    with tab4:
        st.subheader("XGBoost")
        learning_rate = st.number_input("learning_rate", 0.001, 1.0, 0.01)
        n_estimators = st.number_input("n_estimators", 10, 500, 50)
        max_depth = st.number_input("max_depth", 1, 20, 5)
        if st.button("–û–±—É—á–∏—Ç—å XGBoost"):
            with st.spinner("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏..."):
                model, metrics, _, _ = xgboost_model(
                    X_train, X_test, y_train, y_test,
                    learning_rate, n_estimators, max_depth, use_cv, cv_folds
                )
                st.dataframe(metrics)
    
    with tab5:
        st.subheader("KNN")
        n_neighbors = st.number_input("n_neighbors", 1, 50, 5)
        if st.button("–û–±—É—á–∏—Ç—å KNN"):
            with st.spinner("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏..."):
                model, metrics, _, _ = knn_classifier(
                    X_train, X_test, y_train, y_test,
                    n_neighbors, use_cv, cv_folds
                )
                st.dataframe(metrics)
    
    with tab6:
        st.subheader("–ú–µ—Ç–æ–¥ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤")
        if st.button("–û–±—É—á–∏—Ç—å SVM"):
            with st.spinner("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏..."):
                model, metrics, _, _ = svm_classifier(
                    X_train, X_test, y_train, y_test,
                    use_cv, cv_folds
                )
                st.dataframe(metrics)
    
    with tab7:
        st.subheader("–ü–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω")
        layers = st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤", 1, 5, 2)
        neurons = st.number_input("–ù–µ–π—Ä–æ–Ω–æ–≤ –≤ —Å–ª–æ–µ", 16, 256, 64)
        learning_rate = st.number_input("learning_rate", 0.0001, 1.0, 0.01)
        epochs = st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö", 1, 100, 10)
        if st.button("–û–±—É—á–∏—Ç—å Perceptron"):
            with st.spinner("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏..."):
                model, metrics, _, _ = perceptron_classifier(
                    X_train, X_test, y_train, y_test,
                    layers, neurons, learning_rate, epochs, use_cv, cv_folds
                )
                st.dataframe(metrics)
# ======== Models row ============
col1, col2, col3, col4, col5, col6, col7 = st.columns(7)
with col1:
  st.subheader("Logistic Regression")
  run_logreg = st.checkbox("–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –†–µ–≥—Ä–µ—Å—Å–∏–∏!")
  if run_logreg:
    model, metrics_df, y_train_pred, y_test_pred  = logistic_regression(X_train, X_test, y_train, y_test)
    logreg_model = model
    st.subheader("Metrics")
    st.dataframe(metrics_df)

    st.subheader("Train predictions")
    st.dataframe(pd.Series(y_train_pred).head())

    st.subheader("Test predictions")
    st.dataframe(pd.Series(y_test_pred).head())

with col2:
  st.subheader("Decision Tree")
  deps = st.text_input("max_depth", value = 10)
  minsamples = st.text_input("min_samples", value = 10)
  run_tree = st.checkbox("–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å –¥–µ—Ä–µ–≤–∞!")
  if deps and minsamples and run_tree:
    model, metrics_df, y_train_pred, y_test_pred = tree(X_train, X_test, y_train, y_test, max_depth_target = eval(deps), min_samples_split_target = eval(minsamples))
    tree_model = model
    st.subheader("Metrics")
    st.dataframe(metrics_df)

    st.subheader("Train predictions")
    st.dataframe(pd.Series(y_train_pred).head())

    st.subheader("Test predictions")
    st.dataframe(pd.Series(y_test_pred).head())

with col3:
  st.subheader("Random Forest")
  rf_estimators = st.text_input("RF_estimators", value = 50)
  rf_deps = st.text_input("RF_max_depth", value = 10)
  rf_minsamples = st.text_input("RF_min_samples", value = 10)
  run_rf = st.checkbox("–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å –°–ª—É—á–∞–π–Ω–æ–≥–æ –õ–µ—Å–∞!")
  if rf_estimators and rf_deps and rf_minsamples and run_rf:
    model, metrics_df, y_train_pred, y_test_pred = random_forest(X_train, X_test, y_train, y_test, estimators_target = eval(rf_estimators), max_depth_target = eval(rf_deps), min_samples_split_target = eval(rf_minsamples))
    rf_model = model
    st.subheader("Metrics")
    st.dataframe(metrics_df)

    st.subheader("Train predictions")
    st.dataframe(pd.Series(y_train_pred).head())

    st.subheader("Test predictions")
    st.dataframe(pd.Series(y_test_pred).head())

with col4:
  st.subheader("KNN")
  knn_neighbors = st.text_input("knn_neighbors_target", value = 10)
  run_knn = st.checkbox("–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å KNN!")
  if knn_neighbors and run_knn:
    model, metrics_df, y_train_pred, y_test_pred = knn_classifier(X_train, X_test, y_train, y_test, neighbors_target = eval(knn_neighbors))
    knn_model = model
    st.subheader("Metrics")
    st.dataframe(metrics_df)

    st.subheader("Train predictions")
    st.dataframe(pd.Series(y_train_pred).head())

    st.subheader("Test predictions")
    st.dataframe(pd.Series(y_test_pred).head())

with col5:
  st.subheader("XGBoost")
  xgb_learning_rate = st.text_input("xgb_learning_rate", value = 0.01)
  xgb_estimators = st.text_input("XGB_min_samples", value = 50)
  xgb_deps = st.text_input("XGB_max_depth", value = 10)
  run_xgb = st.checkbox("–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å XGBoost!")
  if run_xgb and xgb_learning_rate and xgb_estimators and xgb_deps:
    model, metrics_df, y_train_pred, y_test_pred = xgboost(X_train, X_test, y_train, y_test, learning_rate = eval(xgb_learning_rate), n_estimators = eval(xgb_estimators), max_depth = eval(xgb_deps))
    xgb_model = model
    st.subheader("Metrics")
    st.dataframe(metrics_df)

    st.subheader("Train predictions")
    st.dataframe(pd.Series(y_train_pred).head())

    st.subheader("Test predictions")
    st.dataframe(pd.Series(y_test_pred).head())

with col6:
  st.subheader("SVC")
  run_svc = st.checkbox("–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å SVC!")
  if run_svc:
    model, metrics_df, y_train_pred, y_test_pred = svc(X_train, X_test, y_train, y_test)
    svc_model = model
    st.subheader("Metrics")
    st.dataframe(metrics_df)

    st.subheader("Train predictions")
    st.dataframe(pd.Series(y_train_pred).head())

    st.subheader("Test predictions")
    st.dataframe(pd.Series(y_test_pred).head())


with col7:
  st.subheader("Perceptron")
  p_layers_target = st.text_input("layers_target", value = 2)
  p_neurons_target = st.text_input("neurons_target", value = 50)
  p_learning_rate_target = st.text_input("learning_rate_target", value = 0.01)
  p_epochs_target = st.text_input("epochs_target", value = 10)
  run_perceptron = st.checkbox("–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å –ù–µ–∏—Ä–æ—Å–µ—Ç—å!")
  if p_layers_target and p_neurons_target and p_learning_rate_target and p_epochs_target and run_perceptron:
    model, metrics_df, y_train_pred, y_test_pred =  perceptron_classifier(X_train, X_test, y_train, y_test,
                          layers_target = eval(p_layers_target), neurons_target = eval(p_neurons_target), learning_rate_target = eval(p_learning_rate_target),
                          epochs_target = eval(p_epochs_target))
    perceptron_model = model
    st.subheader("Metrics")
    st.dataframe(metrics_df)

    st.subheader("Train predictions")
    st.dataframe(pd.Series(y_train_pred).head())

    st.subheader("Test predictions")
    st.dataframe(pd.Series(y_test_pred).head())

# ======= –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ =======
available_functions = {
    '–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è': logistic_regression,
    '–î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π': tree,
    '–†–∞–Ω–¥–æ–º–Ω—ã–π –ª–µ—Å': random_forest,
    'XGboost': xgboost,
    '–ú–µ—Ç–æ–¥ –û–ø–æ—Ä–Ω—ã—Ö –í–µ–∫—Ç–æ—Ä–æ–≤': svc,
    'KNN-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä': knn_classifier,
    '–ü–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä': perceptron_classifier
}

use_cv = st.sidebar.checkbox("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é", value=False)
cv_folds = st.sidebar.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤", 2, 10, 5)


