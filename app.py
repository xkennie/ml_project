### OVERALL CODE
#–ª–∏–±—ã
#–±–∞–∑–∞
#!pip install streamlit
#!pip install ydata-profiling

import os
import numpy as np
import pandas as pd
import streamlit as st
from sklearn.model_selection import train_test_split
#from ydata_profiling import ProfileReport
import inspect
from collections import Counter
import seaborn as sns
import matplotlib.pyplot as plt
#classification models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import VotingClassifier, StackingClassifier
from collections import Counter
from sklearn.preprocessing import LabelEncoder, StandardScaler
import keras
from tensorflow.keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import to_categorical
st.set_page_config(page_title="HSE | –¢–ò–ò–ü–ê", layout="wide")

st.title("¬© HSE | –¢–ò–ò–ü–ê")
st.markdown("**Fast-touch classification-analytical tool**")

st.markdown("---")

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–º–∞–Ω–¥–µ
with st.expander("‚Ñπ–û –∫–æ–º–∞–Ω–¥–µ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤"):
    st.markdown("""
    **–ö–æ–º–∞–Ω–¥–∞ –ø—Ä–æ–µ–∫—Ç–∞:**
    - –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏–Ω –ò–ª—å—è—â–µ–Ω–∫–æ ‚Äî Team Leader
    - –í–∞–¥–∏–º –ö–∞–∑–∞–∫–æ–≤ ‚Äî Master of Machine Learning
    - –ê–Ω–¥—Ä–µ–π –®–∏—Ä—à–æ–≤ ‚Äî Business Developer
    - –¢–∞—Ç—å—è–Ω–∞ –ß–µ—Ä–Ω—ã—Ö ‚Äî Designer
    """)

# –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
with st.expander("–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —Å–µ—Ä–≤–∏—Å–∞", expanded=True):
    st.markdown("""
    1. *–í—ã–±–µ—Ä–∏—Ç–µ –Ω—É–∂–Ω—É—é —Ç–µ–º—É* –∞–Ω–∞–ª–∏–∑–∞
    2. *–ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –¥–∞–Ω–Ω—ã–µ* —Å –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    3. *–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ* –∏ –≤—ã–±–µ—Ä–∏—Ç–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
    4. *–ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–Ω–∞–ª–∏–∑* ‚Äî –≤—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –¥–æ–≤–µ—Ä—å—Ç–µ—Å—å –Ω–∞–º. *–î–∞–π—Ç–µ –∞–ª–≥–æ—Ä–∏—Ç–º—É –≤—Ä–µ–º—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.*
    5. *–ò–∑—É—á–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã* ‚Äî –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.
    6. *–í—ã–±–µ—Ä–∏—Ç–µ* –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –∏ *–ø—Ä–∏–º–µ–Ω–∏—Ç–µ* –µ—ë –Ω–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    """)

st.markdown("---")

#st.title("–ó–∞–≥—Ä—É–∑–∏ —Ñ–∞–π–ª —Å—é–¥–∞, –¥—Ä—É–≥")
#uploaded_file = st.file_uploader("Select a CSV file", type=["csv"])
st.title("–î–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª (CSV –∏–ª–∏ Excel)", type=["csv", "xls", "xlsx"])

if uploaded_file is not None:
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
    file_name = uploaded_file.name.lower()

    if file_name.endswith('.csv'):
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è CSV —Ñ–∞–π–ª–æ–≤
        sep_sign = st.selectbox(
            "–í—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å",
            (";", ",", " ", "|"), index=0)

        decimal_sign = st.selectbox(
            "–í—ã–±–µ—Ä–∏—Ç–µ –æ—Ç–¥–µ–ª–∏—Ç–µ–ª—å –¥—Ä–æ–±–Ω–æ–π —á–∞—Å—Ç–∏",
            (".", ","), index=1)

        df = pd.read_csv(uploaded_file, sep=sep_sign, decimal=decimal_sign)

    elif file_name.endswith(('.xls', '.xlsx')):
        df = pd.read_excel(uploaded_file)

    st.write("–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö:")
    st.dataframe(df.head())
    backup_df = df.copy()
 
#if uploaded_file is not None:
  #seps = [";", ","]
  #decimals = [".", ","]
  #sep_sign = ";"
  #sep_sign = st.selectbox(
   # "–í—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å",
   # (";", ",", " ", "/"))
  #decimal_sign = ","
  #decimal_sign = st.selectbox(
    #"–í—ã–±–µ—Ä–∏—Ç–µ –æ—Ç–¥–µ–ª–∏—Ç–µ–ª—å –¥—Ä–æ–±–Ω–æ–π —á–∞—Å—Ç–∏",
    #(".", ","))
  #df = pd.read_csv(uploaded_file, sep = sep_sign, decimal = decimal_sign)
  #st.write("–¢–≤–æ–π –¥–∞—Ç–∞—Å–µ—Ç:")
  #st.dataframe(df.head())
  #backup_df = df.copy()


#—á—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
#def read_data(df):
  #return df
#def handle_missing_values(df, option):

    #df_processed = df.copy()
    
    #if option == "–î—Ä–æ–ø–Ω—É—Ç—å":
        #df_processed = df_processed.dropna()
    #elif option == "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –Ω–æ–ª—å":
        #for col in df_processed.columns:
            #if pd.api.types.is_numeric_dtype(df_processed[col]):
                #df_processed[col] = df_processed[col].fillna(0)
            #else:
                #df_processed[col] = df_processed[col].fillna("0")
    #elif option == "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –º–µ–¥–∏–∞–Ω—É":
        #for col in df_processed.columns:
            #if pd.api.types.is_numeric_dtype(df_processed[col]):
                #median_val = df_processed[col].median()
                #df_processed[col] = df_processed[col].fillna(median_val)
            #else:
                #df_processed[col] = df_processed[col].fillna("0")
    
    #return df_processed
# –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ Streamlit
def handle_missing_values(df, option, categorical_method="mode"):
    df_processed = df.copy()

    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ –ø—Ä–∏–º–µ–Ω—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
    for col in df_processed.columns:
        # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if pd.api.types.is_numeric_dtype(df_processed[col]):
            if option == "–£–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏":
                df_processed = df_processed.dropna(subset=[col])
            elif option == "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –Ω–æ–ª—å":
                df_processed[col] = df_processed[col].fillna(0)
            elif option == "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –º–µ–¥–∏–∞–Ω—É":
                df_processed[col] = df_processed[col].fillna(df_processed[col].median())
            elif option == "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Å—Ä–µ–¥–Ω–µ–µ":
                df_processed[col] = df_processed[col].fillna(df_processed[col].mean())
            elif option == "–ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è":
                df_processed[col] = df_processed[col].interpolate()

        # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        else:
            if option == "–£–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏":
                df_processed = df_processed.dropna(subset=[col])
            elif option == "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –º–æ–¥—É":
                df_processed[col] = df_processed[col].fillna(df_processed[col].mode()[0])
            elif option == "–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é 'Unknown'":
                df_processed[col] = df_processed[col].fillna("Unknown")
            elif option == "–≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è (–ø–æ—Å–ª–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)":
                df_processed[col] = df_processed[col].fillna(method='fill')

    return df_processed
st.title("–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ")

# –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é df
if 'df' not in globals():
    st.warning("–î–∞—Ç–∞—Ñ—Ä–µ–π–º –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–∞–º—è—Ç–∏. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ.")
    st.stop()
# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–ø—É—Å–∫–∞—Ö
st.subheader("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
missing_stats = df.isna().sum()
missing_percent = (missing_stats / len(df)) * 100
missing_df = pd.DataFrame({
    "–ö–æ–ª–æ–Ω–∫–∞": missing_stats.index,
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤": missing_stats.values,
    "–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤": missing_percent.values.round(2)
})
st.dataframe(missing_df)

# –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
st.markdown("---")
st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤")

col1, col2 = st.columns(2)
with col1:
    method = st.selectbox(
        "–ú–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤:",
        options=[
            "–£–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏",
            "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –Ω–æ–ª—å",
            "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –º–µ–¥–∏–∞–Ω—É",
            "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Å—Ä–µ–¥–Ω–µ–µ",
            "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –º–æ–¥—É",
            "–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é 'Unknown'",
            "–ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è",
            "–≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è (–ø–æ—Å–ª–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)"
        ],
        index=2,
        help="–í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"
    )

with col2:
    show_details = st.checkbox("–ü–æ–∫–∞–∑–∞—Ç—å –ø–æ—è—Å–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤", value=True)

if show_details:
    with st.expander("üìö –ü–æ—è—Å–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏"):
        st.markdown("""
        - **–£–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏** - –ø–æ–ª–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –ø—Ä–æ–ø—É—Å–∫–∏
        - **–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –Ω–æ–ª—å** - –∑–∞–º–µ–Ω–∞ –≤—Å–µ—Ö –ø—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–∞ 0 (–¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö)
        - **–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –º–µ–¥–∏–∞–Ω—É** - –∑–∞–º–µ–Ω–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –º–µ–¥–∏–∞–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º (—É—Å—Ç–æ–π—á–∏–≤–æ –∫ –≤—ã–±—Ä–æ—Å–∞–º)
        - **–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Å—Ä–µ–¥–Ω–µ–µ** - –∑–∞–º–µ–Ω–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º (–º–æ–∂–µ—Ç –∏—Å–∫–∞–∂–∞—Ç—å—Å—è –≤—ã–±—Ä–æ—Å–∞–º–∏)
        - **–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –º–æ–¥—É** - –∑–∞–º–µ–Ω–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º (–¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)
        - **–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é 'Unknown'** - –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–ø—É—Å–∫–µ
        - **–ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è** - –ª–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π (–¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤)
        - **–≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è** - –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏–º –∏–∑–≤–µ—Å—Ç–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º (–º–µ—Ç–æ–¥ 'forward fill')
        """)

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
if st.checkbox("–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è"): #, type="primary"):
    with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö..."):
        df = handle_missing_values(df, method)
        st.success("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        st.markdown("---")
        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏")

        col1, col2 = st.columns(2)
        with col1:
            st.metric("–ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫", len(backup_df))
        with col2:
            st.metric("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏", len(df))

        st.dataframe(df.head(10))
# –í—ã–ø–∞–¥–∞—é—â–∏–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–µ—Ç–æ–¥–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤
#missing_values_option = st.selectbox(
    #"–ß—Ç–æ –¥–µ–ª–∞—Ç—å —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏?",
    #options=["–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –Ω–æ–ª—å", "–î—Ä–æ–ø–Ω—É—Ç—å", "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –º–µ–¥–∏–∞–Ω—É"],
    #index=0  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—ã–±—Ä–∞–Ω "–ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –Ω–æ–ª—å"
#)

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
#df = handle_missing_values(df, missing_values_option)

# –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
#st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏")
#st.write(f"–í—ã–±—Ä–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥: {missing_values_option}")
#st.write(f"–ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {len(backup_df)}")
#st.write(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(df)}")
#df = st.session_state.df
# –ü–æ–∫–∞–∑–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
st.dataframe(df)

#–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ —Ä–∞–±–æ—Ç–æ–π
st.title("–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö")

st.write(df.describe())

st.header("–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑")

# –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
st.title("–í–∏–∑—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö")
st.markdown("---")

with st.expander("–û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö", expanded=True):
    st.subheader("–û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")

    # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ describe()
    desc = df.describe().T
    desc['missing'] = df.isna().sum()
    desc['missing_percent'] = (desc['missing'] / len(df)).round(2)
    desc['dtype'] = df.dtypes
    desc['unique'] = df.nunique()

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤—ã–≤–æ–¥
    st.dataframe(
        desc.style.format({
            'mean': '{:.2f}',
            'std': '{:.2f}',
            'min': '{:.2f}',
            '25%': '{:.2f}',
            '50%': '{:.2f}',
            '75%': '{:.2f}',
            'max': '{:.2f}',
            'missing_percent': '{:.0%}'
        }),
        use_container_width=True
    )

st.markdown("---")
st.header("–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è")

# –í—ã–±–æ—Ä —Ç–∏–ø–∞ –≥—Ä–∞—Ñ–∏–∫–∞
chart_type = st.radio(
    "–¢–∏–ø –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:",
    ["–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞", "–Ø—â–∏–∫ —Å —É—Å–∞–º–∏ (Boxplot)", "–ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"],
    horizontal=True
)

col1, col2 = st.columns([3, 1])
with col1:
    selected_col = st.selectbox(
        "–í—ã–±–µ—Ä–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:",
        options=df.select_dtypes(include=['number']).columns,
        index=0
    )

with col2:
    if chart_type == "–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞":
        bins = st.slider(
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤:",
            min_value=5,
            max_value=50,
            value=int(np.sqrt(len(df)) if len(df) > 0 else 20)
        )

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞
plt.style.use('dark_background')
fig, ax = plt.subplots(figsize=(10, 5))

if chart_type == "–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞":
    sns.histplot(df[selected_col], bins=bins, kde=True, ax=ax)
    ax.set_title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {selected_col}')
    ax.set_xlabel(selected_col)
    ax.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
elif chart_type == "–Ø—â–∏–∫ —Å —É—Å–∞–º–∏ (Boxplot)":
    sns.boxplot(x=df[selected_col], ax=ax)
    ax.set_title(f'Boxplot –¥–ª—è {selected_col}')
elif chart_type == "–ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è":
    sns.kdeplot(df[selected_col], ax=ax, fill=True)
    ax.set_title(f'–ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è {selected_col}')
    ax.set_xlabel(selected_col)
    ax.set_ylabel('–ü–ª–æ—Ç–Ω–æ—Å—Ç—å')

st.pyplot(fig)

st.markdown("---")
    # 1. –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏
#st.subheader("–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞")
#selected_col = st.selectbox(
#        "–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫—É –¥–ª—è –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã",
#        options=df.columns,
#        index=2
#    )
#hist_values = np.histogram(df[selected_col], bins=int(round( len(df[selected_col])**0.5 ,0)))
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame –¥–ª—è st.bar_chart
#hist_df = pd.DataFrame({
        #'bin_left': hist_values[1][:-1],
        #'bin_right': hist_values[1][1:],
        #'count': hist_values[0]
    #})
#hist_df['bin'] = hist_df.apply(lambda x: f"{x['bin_left']:.2f}-{x['bin_right']:.2f}", axis=1)
    
# –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –±–∞—Ä—á–∞—Ä—Ç
#st.bar_chart(hist_df.set_index('bin')['count'])
    
    # 2. Scatter plot —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
st.subheader("Scatter plot")
col1, col2, col3, col4 = st.columns(4)
    
with col1:
  x_axis = st.selectbox(
            "–û—Å—å X",
            options=df.columns,
            index=0
        )
with col2:
  y_axis = st.selectbox(
            "–û—Å—å Y",
            options=df.columns,
            index=1 if len(df.columns) > 1 else 0
        )
    
with col3:
  color_col = st.selectbox(
            "–¶–≤–µ—Ç",
            options=["None"] + list(df.columns),
            index=0
        )
with col4:
  size_col = st.selectbox(
            "–†–∞–∑–º–µ—Ä",
            options=["None"] + list(df.columns),
            index=0
        )    
if (color_col == "None") & (size_col == "None"):
    st.scatter_chart(df, x=x_axis, y=y_axis)
elif (size_col == "None") & (color_col != "None"):
    st.scatter_chart(df, x=x_axis, y=y_axis, color=color_col)
elif (size_col != "None") & (color_col == "None"):
    st.scatter_chart(df, x=x_axis, y=y_axis, size = size_col)
elif (size_col != "None") & (color_col != "None"):
    st.scatter_chart(df, x=x_axis, y=y_axis, color = color_col, size = size_col)

#st.subheader("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è")
#    
#    # 1. –í—ã–±–æ—Ä —Ç–∞—Ä–≥–µ—Ç–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (y)
#target_col = st.selectbox(
#        "–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∞—Ä–≥–µ—Ç–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (y)",
#        options=df.columns,
#        index=0,  # –î–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ - –ø–µ—Ä–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞
#        key="target_select"
#    )
    
#    # 2. –í—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (X) - –∏—Å–∫–ª—é—á–∞–µ–º —Ç–∞—Ä–≥–µ—Ç –∏–∑ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
#available_features = [col for col in df.columns if col != target_col]
    
#selected_features = st.multiselect(
#        "–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏ (X)",
#        options=available_features,
#        default=available_features,  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—ã–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
#        key="features_select"
#    )
#    
#    # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –≤—ã–±—Ä–∞–Ω—ã —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –ø—Ä–∏–∑–Ω–∞–∫
#if not selected_features:
#  st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –ø—Ä–∏–∑–Ω–∞–∫ –¥–ª—è –º–æ–¥–µ–ª–∏")
#      
#if selected_features:    
#    # –§–æ—Ä–º–∏—Ä—É–µ–º X –∏ y
#  y = df[target_col]
#  X = df[selected_features]
#    
#    # 3. –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ train/test
#  X_train, X_test, y_train, y_test = train_test_split(
#        X, y, 
#        test_size=0.2, 
#        random_state=42
#    )
# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—é
st.subheader("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è")
st.markdown("---")

# 1. –í—ã–±–æ—Ä —Ç–∞—Ä–≥–µ—Ç–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (y)
target_col = st.selectbox(
    "–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∞—Ä–≥–µ—Ç–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (y)",
    options=df.columns,
    index=0,
    key="target_select"
)

# 2. –í—ã–±–æ—Ä ID –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (–¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π)
id_cols = st.multiselect(
    "–í—ã–±–µ—Ä–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ-–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã (–Ω–µ –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤ –º–æ–¥–µ–ª–∏)",
    options=[col for col in df.columns if col != target_col],
    key="id_cols"
)

# 3. –í—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (X)
available_features = [col for col in df.columns if col != target_col and col not in id_cols]

selected_features = st.multiselect(
    "–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏ (X)",
    options=available_features,
    default=available_features,
    key="features_select"
)

if not selected_features:
    st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –ø—Ä–∏–∑–Ω–∞–∫ –¥–ª—è –º–æ–¥–µ–ª–∏")
    st.stop()

# 4. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
with st.expander("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö", expanded=True):
    st.subheader("–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")
    numeric_cols = [col for col in selected_features if pd.api.types.is_numeric_dtype(df[col])]
    norm_cols = st.multiselect(
        "–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (StandardScaler)",
        options=numeric_cols,
        help="–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é (z-score)"
    )

    st.subheader("–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ")
    log_cols = st.multiselect(
        "–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏—è",
        options=numeric_cols,
        help="–ü—Ä–∏–º–µ–Ω—è–µ—Ç log(x+1) –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø—Ä–∞–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω–µ-—Å–∫–æ—à–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
    )

    st.subheader("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    categorical_cols = [col for col in selected_features if not pd.api.types.is_numeric_dtype(df[col])]
    dummy_cols = st.multiselect(
        "–í—ã–±–µ—Ä–∏—Ç–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è one-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è",
        options=categorical_cols,
        help="–°–æ–∑–¥–∞—Å—Ç dummy-–ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (n-1) –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
    )

    st.subheader("–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤")
    if pd.api.types.is_numeric_dtype(df[target_col]):
        st.warning("–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–≥–æ —Ç–∞—Ä–≥–µ—Ç–∞")
    else:
        balance_method = st.selectbox(
            "–ú–µ—Ç–æ–¥ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤",
            options=["–ù–µ—Ç", "Random Oversampling", "SMOTE", "Random Undersampling"],
            index=0,
            help="–í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ"
        )

# 5. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∞–∑–±–∏–µ–Ω–∏—è
test_size = st.slider(
    "–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (%)",
    min_value=5,
    max_value=40,
    value=20,
    step=5
)

random_state = st.number_input(
    "Random state –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏",
    min_value=0,
    value=42
)
    
    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–±–∏–µ–Ω–∏–∏
#  st.success("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã!")
#  st.write(f"–í—ã–±—Ä–∞–Ω —Ç–∞—Ä–≥–µ—Ç: {target_col}")
#  st.write(f"–í—ã–±—Ä–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(selected_features)}")
#  st.write(f"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {X_train.shape[0]}")
#  st.write(f"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_test.shape[0]}")
def preprocess_data(data, target_col, id_cols, features, norm_cols, log_cols, dummy_cols,
                   balance_method, test_size, random_state):

    # –í—ã–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ç–∞—Ä–≥–µ—Ç
    X = data[features]
    y = data[target_col]
    ids = data[id_cols] if id_cols else None

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=test_size/100,
        random_state=random_state,
        stratify=y if not pd.api.types.is_numeric_dtype(y) else None
    )

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º ID –¥–ª—è train –∏ test
    if ids is not None:
        train_ids = ids.loc[X_train.index]
        test_ids = ids.loc[X_test.index]
    else:
        train_ids, test_ids = None, None

    # 1. –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏–µ
    for col in log_cols:
        X_train[col] = np.log1p(X_train[col])
        X_test[col] = np.log1p(X_test[col])

    # 2. One-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
    if dummy_cols:
        encoder = OneHotEncoder(drop='first', sparse=False)
        train_encoded = encoder.fit_transform(X_train[dummy_cols])
        test_encoded = encoder.transform(X_test[dummy_cols])

        encoded_cols = encoder.get_feature_names_out(dummy_cols)
        train_encoded_df = pd.DataFrame(train_encoded, columns=encoded_cols, index=X_train.index)
        test_encoded_df = pd.DataFrame(test_encoded, columns=encoded_cols, index=X_test.index)

        X_train = X_train.drop(dummy_cols, axis=1).join(train_encoded_df)
        X_test = X_test.drop(dummy_cols, axis=1).join(test_encoded_df)

    # 3. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    if norm_cols:
        scaler = StandardScaler()
        X_train[norm_cols] = scaler.fit_transform(X_train[norm_cols])
        X_test[norm_cols] = scaler.transform(X_test[norm_cols])

    # 4. –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
    if balance_method != "–ù–µ—Ç" and not pd.api.types.is_numeric_dtype(y_train):
        if balance_method == "Random Oversampling":
            sampler = RandomOverSampler(random_state=random_state)
        elif balance_method == "SMOTE":
            sampler = SMOTE(random_state=random_state)
        elif balance_method == "Random Undersampling":
            sampler = RandomUnderSampler(random_state=random_state)

        X_train, y_train = sampler.fit_resample(X_train, y_train)

    return X_train, X_test, y_train, y_test, train_ids, test_ids

# –ö–Ω–æ–ø–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
if st.checkbox("–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ"):
    with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö..."):
        # –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º
        X_train, X_test, y_train, y_test, train_ids, test_ids = preprocess_data(
            data=df,
            target_col=target_col,
            id_cols=id_cols,
            features=selected_features,
            norm_cols=norm_cols,
            log_cols=log_cols,
            dummy_cols=dummy_cols,
            balance_method=balance_method,
            test_size=test_size,
            random_state=random_state
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ session_state
        #st.session_state.update({
            #'X_train': X_train,
            #'X_test': X_test,
            #'y_train': y_train,
            #'y_test': y_test,
            #'train_ids': train_ids,
            #'test_ids': test_ids
        #})

        st.success("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã!")

        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        st.markdown("---")
        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏")

        col1, col2 = st.columns(2)
        with col1:
            st.metric("–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞", f"{len(X_train)} –Ω–∞–±–ª—é–¥–µ–Ω–∏–π")
            st.metric("–ü—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏", X_train.shape[1])
            if hasattr(y_train, 'value_counts'):
                st.write("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ (train):")
                st.dataframe(y_train.value_counts(normalize=True))

        with col2:
            st.metric("–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞", f"{len(X_test)} –Ω–∞–±–ª—é–¥–µ–Ω–∏–π")
            st.metric("–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–∞", f"{test_size}%")
            if hasattr(y_test, 'value_counts'):
                st.write("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ (test):")
                st.dataframe(y_test.value_counts(normalize=True))

        # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
        st.markdown("---")
        st.subheader("–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö")

        tab1, tab2 = st.tabs(["–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞", "–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞"])
        with tab1:
            display_df = X_train.head(5).copy()
            display_df[target_col] = y_train.head(5).values
            if train_ids is not None:
                display_df = pd.concat([train_ids.head(5), display_df], axis=1)
            st.dataframe(display_df)

        with tab2:
            display_df = X_test.head(5).copy()
            display_df[target_col] = y_test.head(5).values
            if test_ids is not None:
                display_df = pd.concat([test_ids.head(5), display_df], axis=1)
            st.dataframe(display_df)

# –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ ML –º–æ–¥–µ–ª—è—Ö
#if 'X_train' in st.session_state:
    #X_train = st.session_state.X_train
    #X_test = st.session_state.X_test
    #y_train = st.session_state.y_train
    #y_test = st.session_state.y_test
if 'X_train' not in globals():
    st.warning("–ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–∞–Ω–Ω—ã–µ")
    st.stop()
else:
    X_train = X_train
    X_test = X_test
    y_train = y_train
    y_test = y_test


def plot_confusion_matrix(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    fig, ax = plt.subplots(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)
    ax.set_title(title)
    ax.set_xlabel('Predicted')
    ax.set_ylabel('Actual')
    st.pyplot(fig)

def evaluate_model(model, X_train, X_test, y_train, y_test, model_name, use_cv=False, cv_folds=5):
    if use_cv:
        scores = cross_val_score(model, X_train, y_train, cv=cv_folds, scoring='accuracy')
        st.write(f"–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è (Accuracy): –°—Ä–µ–¥–Ω–µ–µ = {scores.mean():.4f}, Std = {scores.std():.4f}")
        model.fit(X_train, y_train)
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)
    else:
        model.fit(X_train, y_train)
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)

    metrics = {
        'Accuracy': [
            accuracy_score(y_train, y_train_pred),
            accuracy_score(y_test, y_test_pred)
        ],
        'Precision': [
            precision_score(y_train, y_train_pred, average='weighted'),
            precision_score(y_test, y_test_pred, average='weighted')
        ],
        'Recall': [
            recall_score(y_train, y_train_pred, average='weighted'),
            recall_score(y_test, y_test_pred, average='weighted')
        ],
        'F1': [
            f1_score(y_train, y_train_pred, average='weighted'),
            f1_score(y_test, y_test_pred, average='weighted')
        ]
    }

    metrics_df = pd.DataFrame(metrics, index=['Train', 'Test'])

    st.subheader(f"–ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –¥–ª—è {model_name}")
    col1, col2 = st.columns(2)
    with col1:
        plot_confusion_matrix(y_train, y_train_pred, f'Train\n{model_name}')
    with col2:
        plot_confusion_matrix(y_test, y_test_pred, f'Test\n{model_name}')

    return model, metrics_df, y_train_pred, y_test_pred  
models = {}
#logre–ø
def logistic_regression(X_train, X_test, y_train, y_test, use_cv=False, cv_folds=5):
  logreg = LogisticRegression(
    multi_class='multinomial',
    max_iter = 1000
)
  logreg.fit(X_train, y_train)
  return evaluate_model(logreg, X_train, X_test, y_train, y_test, "Logistic Regression", use_cv=use_cv, cv_folds=cv_folds)

#tree
def tree(X_train, X_test, y_train, y_test, max_depth_target = 10, min_samples_split_target = 10, use_cv=False, cv_folds=5):
  tree = DecisionTreeClassifier(
    max_depth = max_depth_target,             # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞
    min_samples_split = min_samples_split_target,     # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
)
  tree.fit(X_train, y_train)
  return evaluate_model(tree, X_train, X_test, y_train, y_test, "Decision Tree", use_cv=use_cv, cv_folds=cv_folds)

#forest
def random_forest(X_train, X_test, y_train, y_test, estimators_target = 50, max_depth_target = 10, min_samples_split_target = 10, use_cv=False, cv_folds=5):
  random_forest = RandomForestClassifier(
    n_estimators = estimators_target,  # –ß–∏—Å–ª–æ –¥–µ—Ä–µ–≤—å–µ–≤
    max_features='sqrt',
    max_depth = max_depth_target,
    min_samples_split = min_samples_split_target
)
  random_forest.fit(X_train, y_train)
  return evaluate_model(random_forest, X_train, X_test, y_train, y_test, "Random Forest", use_cv=use_cv, cv_folds=cv_folds)


#xgboost
def xgboost(X_train, X_test, y_train, y_test, learning_rate=0.01, n_estimators=50, max_depth=10, use_cv=False, cv_folds=5):
    le = LabelEncoder()
    y_train_encoded = le.fit_transform(y_train)
    y_test_encoded = le.transform(y_test)

    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    xgboost_model = XGBClassifier(
        learning_rate=learning_rate,
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=42,
        eval_metric='mlogloss',  # –î–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        use_label_encoder=False  # –ß—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
    )

    if use_cv:
        scores = cross_val_score(xgboost_model, X_train, y_train_encoded, cv=cv_folds, scoring='accuracy')
        st.write(f"–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è (Accuracy): –°—Ä–µ–¥–Ω–µ–µ = {scores.mean():.4f}, Std = {scores.std():.4f}")

    xgboost_model.fit(X_train, y_train_encoded)

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_train_pred = le.inverse_transform(xgboost_model.predict(X_train))
    y_test_pred = le.inverse_transform(xgboost_model.predict(X_test))

    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
    metrics = {
        'Accuracy': [
            accuracy_score(y_train, y_train_pred),
            accuracy_score(y_test, y_test_pred)
        ],
        'Precision': [
            precision_score(y_train, y_train_pred, average='weighted'),
            precision_score(y_test, y_test_pred, average='weighted')
        ],
        'Recall': [
            recall_score(y_train, y_train_pred, average='weighted'),
            recall_score(y_test, y_test_pred, average='weighted')
        ],
        'F1': [
            f1_score(y_train, y_train_pred, average='weighted'),
            f1_score(y_test, y_test_pred, average='weighted')
        ],
        'ROC-AUC': [
            roc_auc_score(
                pd.get_dummies(y_train),
                xgboost_model.predict_proba(X_train),
                multi_class='ovr',
                average='weighted'
            ) if len(np.unique(y_train_encoded)) > 2 else np.nan,
            roc_auc_score(
                pd.get_dummies(y_test),
                xgboost_model.predict_proba(X_test),
                multi_class='ovr',
                average='weighted'
            ) if len(np.unique(y_test_encoded)) > 2 else np.nan
        ]
    }

    metrics_df = pd.DataFrame(metrics, index=['Train', 'Test'])

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    st.subheader("–ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –¥–ª—è XGBoost")
    col1, col2 = st.columns(2)
    with col1:
        plot_confusion_matrix(y_train, y_train_pred, 'Train\nXGBoost')
    with col2:
        plot_confusion_matrix(y_test, y_test_pred, 'Test\nXGBoost')

    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    st.subheader("–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    fig, ax = plt.subplots(figsize=(10, 6))
    xgb.plot_importance(xgboost_model, ax=ax)
    st.pyplot(fig)

    # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    train_results = pd.DataFrame({
        'index': X_train.index,
        'Actual': y_train,
        'Predicted': y_train_pred
    })

    test_results = pd.DataFrame({
        'index': X_test.index,
        'Actual': y_test,
        'Predicted': y_test_pred
    })

    return xgboost_model, metrics_df, y_train_pred, y_test_pred

#Support-vector-calc
def svc(X_train, X_test, y_train, y_test, use_cv=False, cv_folds=5):
  svc = SVC()
  svc.fit(X_train, y_train)
  return evaluate_model(svc, X_train, X_test, y_train, y_test, "SVC", use_cv=use_cv, cv_folds=cv_folds)

#knn
def knn_classifier(X_train, X_test, y_train, y_test, neighbors_target = 10, use_cv=False, cv_folds=5):
  knn = KNeighborsClassifier(n_neighbors= neighbors_target)
  knn.fit(X_train, y_train)
  return evaluate_model(knn, X_train, X_test, y_train, y_test, "KNN", use_cv=use_cv, cv_folds=cv_folds)

def perceptron_classifier(X_train, X_test, y_train, y_test,
                          layers_target = 2, neurons_target = 50, learning_rate_target = 0.01,
                          epochs_target = 10):
  X_train, X_test, y_train, y_test = preprocess(X_train, X_test, y_train, y_test)
  #y_train encode
  label_encoder = LabelEncoder()
  y_encoded = label_encoder.fit_transform(y_train)
  y_train = to_categorical(y_encoded)  
  #y_test encode
  y_encoded = label_encoder.fit_transform(y_test)
  y_test = to_categorical(y_encoded)

  perceptron = Sequential()
  for n in range(layers_target):
    perceptron.add(Dense(neurons_target, input_dim = X_train.shape[1], activation = 'relu'))  # –°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏
  
  perceptron.add(Dense(y_train.shape[1], activation = 'softmax'))  # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π


  perceptron.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate = learning_rate_target), metrics=['accuracy'])

  perceptron.fit(X_train, y_train, epochs= epochs_target, batch_size=10, validation_data=(X_test, y_test))

  predictions = perceptron.predict(X_test)
  predicted_classes = np.argmax(predictions, axis=1)
  predicted_styles = label_encoder.inverse_transform(predicted_classes)
  predict = pd.Series(predicted_styles)

  true_class = np.argmax(y_test, axis = 1)
  true_styles = label_encoder.inverse_transform(true_class)
  y_true = pd.Series(true_styles)

  perceptron_predicts = pd.concat([y_true, predict], axis = 1)
  perceptron_predicts.columns = ['Style', 'Perceptron_predict']
  perceptron_predicts["index"] = [i for i in range(perceptron_predicts.shape[0])]
  perceptron_predicts = perceptron_predicts[["index", "Style", "Perceptron_predict"]]                        
  return perceptron_predicts
# ======== Models row ============
col1, col2, col3, col4, col5, col6, col7 = st.columns(7)
with col1:
  st.subheader("Logistic Regression")
  run_logreg = st.checkbox("–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –†–µ–≥—Ä–µ—Å—Å–∏–∏!")
  if run_logreg:
    #_, result = logistic_regression(X_train, X_test, y_train, y_test)
    result = logistic_regression(X_train, X_test, y_train, y_test)
    acc = (result['Logreg_predict'] == result['Style']).mean()
    st.write(f"Accuracy: {acc:.2%}")
    st.write(result)
    
with col2:
  st.subheader("Tree")
  deps = st.text_input("max_depth", value = 10)
  minsamples = st.text_input("min_samples", value = 10)
  run_tree = st.checkbox("–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å –¥–µ—Ä–µ–≤–∞!")
  if deps and minsamples and run_tree:
    result = tree(X_train, X_test, y_train, y_test, max_depth_target = eval(deps), min_samples_split_target = eval(minsamples))
    acc = (result['Tree_predict'] == result['Style']).mean()
    st.write(f"Accuracy: {acc:.2%}")
    st.write(result)

    
with col3:
  st.subheader("Random Forest")
  rf_estimators = st.text_input("RF_estimators", value = 50)
  rf_deps = st.text_input("RF_max_depth", value = 10)
  rf_minsamples = st.text_input("RF_min_samples", value = 10)
  run_rf = st.checkbox("–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å –°–ª—É—á–∞–π–Ω–æ–≥–æ –õ–µ—Å–∞!")
  if rf_estimators and rf_deps and rf_minsamples and run_rf:
    result = random_forest(X_train, X_test, y_train, y_test, estimators_target = eval(rf_estimators), max_depth_target = eval(rf_deps), min_samples_split_target = eval(rf_minsamples))
    acc = (result['Random_Forest_predict'] == result['Style']).mean()
    st.write(f"Accuracy: {acc:.2%}")
    st.write(result)

with col4:
  st.subheader("XGBoost")
  xgb_learning_rate = st.text_input("XGB_max_depth", value = 0.01)
  xgb_estimators = st.text_input("XGB_min_samples", value = 50)
  xgb_deps = st.text_input("XGB_max_depth", value = 10)
  run_xgb = st.checkbox("–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å XGBoost!")
  if run_xgb and xgb_learning_rate and xgb_estimators and xgb_deps:
    result = xgboost(X_train, X_test, y_train, y_test, learning_rate_target = eval(xgb_learning_rate), estimators_target = eval(xgb_estimators), max_depth_target = eval(xgb_deps))
    acc = (result['XGBoost_predict'] == result['Style']).mean()
    st.write(f"Accuracy: {acc:.2%}")
    st.write(result)

with col5:
  st.subheader("SVC")
  run_svc = st.checkbox("–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å SVC!")
  if run_svc:
    result = svc(X_train, X_test, y_train, y_test)
    acc = (result['SVC_predict'] == result['Style']).mean()
    st.write(f"Accuracy: {acc:.2%}")
    st.write(result)

with col6:
  st.subheader("KNN")
  knn_neighbors = st.text_input("knn_neighbors_target", value = 10)
  run_knn = st.checkbox("–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å KNN!")
  if knn_neighbors and run_knn:
    result = knn_classifier(X_train, X_test, y_train, y_test, neighbors_target = eval(knn_neighbors))
    acc = (result['KNN_predict'] == result['Style']).mean()
    st.write(f"Accuracy: {acc:.2%}")
    st.write(result)
    
with col7:
  st.subheader("Perceptron")
  p_layers_target = st.text_input("layers_target", value = 2)
  p_neurons_target = st.text_input("neurons_target", value = 50)
  p_learning_rate_target = st.text_input("learning_rate_target", value = 0.01)
  p_epochs_target = st.text_input("epochs_target", value = 10)
  run_perceptron = st.checkbox("–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å –ù–µ–∏—Ä–æ—Å–µ—Ç—å!")
  if p_layers_target and p_neurons_target and p_learning_rate_target and p_epochs_target and run_perceptron:
    result =  perceptron_classifier(X_train, X_test, y_train, y_test,
                          layers_target = eval(p_layers_target), neurons_target = eval(p_neurons_target), learning_rate_target = eval(p_learning_rate_target),
                          epochs_target = eval(p_epochs_target)) 
    acc = (result['Perceptron_predict'] == result['Style']).mean()
    st.write(f"Accuracy: {acc:.2%}")
    st.write(result)

# ======= –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ =======
available_functions = {
    'logistic_regression': logistic_regression,
    'tree': tree,
    'random_forest': random_forest,
    'xgboost': xgboost,
    'svc': svc,
    'knn_classifier': knn_classifier,
    'perceptron_classifier': perceptron_classifier
}

st.title("ML –ê–Ω—Å–∞–º–±–ª—å")

# ======= –í—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π =======
selected_function_names = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏", list(available_functions.keys()))

# ======= –í–≤–æ–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ =======
func_params = {}
for name in selected_function_names:
    func = available_functions[name]
    sig = inspect.signature(func)
    params_to_remove = {'X_train', 'X_test', 'y_train', 'y_test'}
    new_params = {
        name: param 
        for name, param in sig.parameters.items()
        if name not in params_to_remove
    }
    sig = sig.replace(parameters=list(new_params.values()))
    #sig = list(sig)
    #sig = sig.remove(X_train)
    #sig = sig.remove(X_test)
    #sig = sig.remove(y_train)
    #sig = sig.remove(y_test)
    st.subheader(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è {name}")
    params = {}
    for param in sig.parameters.values():
        default_val = "" if param.default is param.empty else param.default
        input_val = st.text_input(f"{name} - {param.name}", value=str(default_val))
        params[param.name] = eval(input_val)
    params["X_train"] = X_train 
    params["X_test"] = X_test 
    params["y_train"] = y_train 
    params["y_test"] = y_test 
    func_params[name] = params
    

# ======= –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞ =======
if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω—Å–∞–º–±–ª—å"):
    merged_df = None
    for name in selected_function_names:
        func = available_functions[name]
        params = func_params.get(name, {})
        result = func(**params)
        if merged_df is None:
            merged_df = result
        else:
            merged_df = pd.merge(merged_df, result, on=['index', 'Style'])

    # ======= –ì–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ =======
    pred_cols = [col for col in merged_df.columns if col.endswith('_predict')]
    def majority_vote(row):
        votes = [row[col] for col in pred_cols]
        return Counter(votes).most_common(1)[0][0]

    merged_df['overall_predict'] = merged_df.apply(majority_vote, axis=1)

    # ======= Accuracy (–µ—Å–ª–∏ –µ—Å—Ç—å —Å—Ç–æ–ª–±–µ—Ü ground truth) =======
    if 'Style' in merged_df.columns:
        acc = (merged_df['overall_predict'] == merged_df['Style']).mean()
        st.write(f"Accuracy: {acc:.2%}")
    
    st.write("–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω—Å–∞–º–±–ª—è:")
    st.dataframe(merged_df[['Style', 'overall_predict']])
